---
title: "Untitled"
output: html_document
---

### Predicting Country-Level Collaborations in Open Source Software (OSS)

In this file, we outline the process for a regression analysis that aim to predict which factors contribute to country-level collaborations in open source software (OSS). Our original OSS dataset (2008-2019) was scraped from GitHub using the `GHOST.jl` package. We created a summary table of all the users, commits, additions, deletions and collaborations at the country-level in our preprocessing steps and then drew from two additional datasources to add some population and technology variables that might help to predict collaboration tendencies. These data come from the World Bank and World Development Index that include variables related to population estimates, technology adoption, gender/income equity, among others. As we show below, we are not able to use all of these variables due to missingness, but we do develop a strong model from the data we do use. 

#### Importing Packages and Data 

First, we load our packages, the OSS data from the database, and the World Bank data on [population estimates](https://datacatalog.worldbank.org/dataset/population-estimates-and-projections) and [World Development Index data on technology adoption and innovation](https://datacatalog.worldbank.org/dataset/world-development-indicators). 

```{r}

# clear our env and load our packages 
rm(list = ls())
library(tidyverse)
library(dplyr)
library(reshape2)
library(readxl)
library(janitor)
library(RPostgreSQL)
library(naniar)
library(mice)
library(caret)
library(VIM)
library(e1071)
library(psych)
library(car)
library(diverstidy)

# next, lets pull in our oss summary data (users, repos, commits, additions, deletions)
# connect to postgresql to get data (in rivanna)
conn <- dbConnect(drv = PostgreSQL(),
                    dbname = "sdad",
                    host = "10.250.124.195",
                    port = 5432,
                    user = Sys.getenv("db_userid"),
                    password = Sys.getenv("db_pwd"))

# query the bipartite edgelist data from github data
ctry_github_activity <- dbGetQuery(conn, str_c("SELECT * FROM gh_sna.desc_intl_ctry_annual_sum"))  
# disconnect from postgresql
dbDisconnect(conn)

# in an earlier file, we ran some country-level network diversity measures (loading them now)
setwd("~/git/oss-2020/data/network-analysis/intl-ctry-nets-cum/wisos-lchn/")
all_diversity_analyses = read_rds("ctry_diversity_cum.rds") 

all_diversity_analyses$year <- plyr::mapvalues(all_diversity_analyses$year, 
          from = c("08", "0809", "0810", "0811", "0812", "0813", "0814", "0815", "0816", "0817", "0818", "0819"), 
          to = c("2008", "2009", "2010", "2011", "2012", "2013", "2014", "2015", "2016", "2017", "2018", "2019"))

all_diversity_analyses <- all_diversity_analyses %>% 
  #rename(country_name = country, net_diversity = diversity) %>% 
  mutate(year = as.numeric(year)) %>% 
  distinct(id, country_name, year, net_diversity, 
           louvain_comm, fstgrdy_comm, louvain_range, fstgrdy_range)
all_diversity_analyses

# these are the data that tell us the number of oss collaborations as well as domestic, international and us collab rates 
setwd("~/git/oss-2020/data/intl-indicator-output/")
collaborations_data <- read_csv("oss_all_intl_collaborations_data_2008_2019.csv") %>% 
  rename(country_name = country)
collaborations_data

# join the activity and collaborations data 
ctry_github_joined = ctry_github_activity %>% 
  filter(year > 2007) %>% 
  left_join(collaborations_data, by = c("country_name", "year")) %>% 
  replace(is.na(.), 0)
ctry_github_joined

# pull in the world bank and world development index data
setwd("~/git/oss-2020/data/wdi-data/")
wdi_data = read_csv("WDIData.csv")
dai_data <- read_excel("DAIforweb.xlsx")
population_estimates <- read_csv("Population-EstimatesData.csv")
```

```{r}
# clean the population data 
population_cleaned <- population_estimates %>% 
  clean_names() %>% 
  filter(indicator_name == "Population, total") %>% 
  select(country_name, x2008:x2019) %>% 
  pivot_longer(cols = starts_with("x"), names_to = "year", values_to = "population") %>% 
  # standardize all of the country names
  mutate(year = str_replace(year, "x", ""),
         year = as.numeric(year),
         country_name = str_replace(country_name, " and ", " & ")) %>% 
  mutate(country_name = ifelse(test = str_detect(string = country_name, pattern = "Bahamas, The"), "Bahamas", no = country_name),
         country_name = ifelse(test = str_detect(string = country_name, pattern = "Congo, Rep."), "Congo - Brazzaville", no = country_name),
         country_name = ifelse(test = str_detect(string = country_name, pattern = "Congo, Dem. Rep."), "Congo - Kinshasa", no = country_name),
         country_name = ifelse(test = str_detect(string = country_name, pattern = "Cote d'Ivoire"), "Côte d’Ivoire", no = country_name),
         country_name = ifelse(test = str_detect(string = country_name, pattern = "Brunei Darussalam"), "Brunei", no = country_name),
         country_name = ifelse(test = str_detect(string = country_name, pattern = "Cabo Verde"), "Cape Verde", no = country_name),
         country_name = ifelse(test = str_detect(string = country_name, pattern = "Czech Republic"), "Czechia", no = country_name),
         country_name = ifelse(test = str_detect(string = country_name, pattern = "Egypt, Arab Rep."), "Egypt", no = country_name),
         country_name = ifelse(test = str_detect(string = country_name, pattern = "Gambia, The"), "Gambia", no = country_name),
         country_name = ifelse(test = str_detect(string = country_name, pattern = "Hong Kong SAR, China"), "Hong Kong SAR China", no = country_name),
         country_name = ifelse(test = str_detect(string = country_name, pattern = "Iran, Islamic Rep."), "Iran", no = country_name),
         country_name = ifelse(test = str_detect(string = country_name, pattern = "Korea, Dem. People’s Rep."), "North Korea", no = country_name),
         country_name = ifelse(test = str_detect(string = country_name, pattern = "Korea, Rep."), "South Korea", no = country_name),
         country_name = ifelse(test = str_detect(string = country_name, pattern = "Kyrgyz Republic"), "Kyrgyzstan", no = country_name),
         country_name = ifelse(test = str_detect(string = country_name, pattern = "Lao PDR"), "Laos", no = country_name),
         country_name = ifelse(test = str_detect(string = country_name, pattern = "Macao SAR, China"), "Macau SAR China", no = country_name),
         country_name = ifelse(test = str_detect(string = country_name, pattern = "Macedonia, FYR"), "Macedonia", no = country_name),
         country_name = ifelse(test = str_detect(string = country_name, pattern = "Micronesia, Fed. Sts."), 
                               "Micronesia (Federated States of)", no = country_name),
         country_name = ifelse(test = str_detect(string = country_name, pattern = "Myanmar"), "Myanmar (Burma)", no = country_name),
         country_name = ifelse(test = str_detect(string = country_name, pattern = "Russian Federation"), "Russia", no = country_name),
         country_name = ifelse(test = str_detect(string = country_name, pattern = "Sao Tome & Principe"), "São Tomé & Príncipe", no = country_name),
         country_name = ifelse(test = str_detect(string = country_name, pattern = "Sint Maarten (Dutch part)"), "Sint Maarten", no = country_name),
         country_name = ifelse(test = str_detect(string = country_name, pattern = "Slovak Republic"), "Slovakia", no = country_name),
         country_name = ifelse(test = str_detect(string = country_name, pattern = "St. Vincent & Grenadines"), 
                               "St. Vincent & the Grenadines", no = country_name),
         country_name = ifelse(test = str_detect(string = country_name, pattern = "Syrian Arab Republic"), "Syria", no = country_name),
         country_name = ifelse(test = str_detect(string = country_name, pattern = "Venezuela, RB"), "Venezuela", no = country_name),
         country_name = ifelse(test = str_detect(string = country_name, pattern = "Yemen, Rep."), "Yemen", no = country_name))

# i had to manually select all of the variables i was interested and then validated this list with gizem 
# i created this list and then had to filter out the rest of the data 
filtered_variables = "CPIA trade rating|Trademark applications, resident, by count|Computer, communications and other services|High-technology exports % of manufactured exports|Investment in ICT with private participation|GDP per capita constant 2010 US|CPIA policies for social inclusion/equity cluster average|CPIA gender equality rating|Gini index World Bank estimate|Research and development expenditure % of GDP|Researchers in R&D per million people|Technicians in R&D per million people|Human capital index HCI scale 0-1|Individuals using the Internet % of population|Secure Internet servers per 1 million people"

filtered_df = wdi_data %>% 
  clean_names() %>% 
  select(country_name, country_code, indicator_name, x2008:x2019) %>% 
  mutate(indicator_name = str_replace(indicator_name, "\\(", ""),
         indicator_name = str_replace(indicator_name, "\\)", ""),
         indicator_name = str_replace(indicator_name, "\\$", ""),
         indicator_name = str_replace(indicator_name, "\\(scale 0-1\\)", "scale 0-1")) %>% 
  filter(grepl(filtered_variables, indicator_name))

filtered_df = filtered_df %>% 
  select(country_name, indicator_name, x2008:x2019) %>% 
  pivot_longer(cols = starts_with("x"), names_to = "year", values_to = "count") %>% 
  mutate(year = str_replace(year, "x", ""),
         year = as.numeric(year),
         country_name = str_replace(country_name, " and ", " & ")) %>% 
  # standardize all of the country names
  mutate(country_name = ifelse(test = str_detect(string = country_name, pattern = "Bahamas, The"), "Bahamas", no = country_name),
         country_name = ifelse(test = str_detect(string = country_name, pattern = "Congo, Rep."), "Congo - Brazzaville", no = country_name),
         country_name = ifelse(test = str_detect(string = country_name, pattern = "Congo, Dem. Rep."), "Congo - Kinshasa", no = country_name),
         country_name = ifelse(test = str_detect(string = country_name, pattern = "Cote d'Ivoire"), "Côte d’Ivoire", no = country_name),
         country_name = ifelse(test = str_detect(string = country_name, pattern = "Brunei Darussalam"), "Brunei", no = country_name),
         country_name = ifelse(test = str_detect(string = country_name, pattern = "Cabo Verde"), "Cape Verde", no = country_name),
         country_name = ifelse(test = str_detect(string = country_name, pattern = "Czech Republic"), "Czechia", no = country_name),
         country_name = ifelse(test = str_detect(string = country_name, pattern = "Egypt, Arab Rep."), "Egypt", no = country_name),
         country_name = ifelse(test = str_detect(string = country_name, pattern = "Gambia, The"), "Gambia", no = country_name),
         country_name = ifelse(test = str_detect(string = country_name, pattern = "Hong Kong SAR, China"), "Hong Kong SAR China", no = country_name),
         country_name = ifelse(test = str_detect(string = country_name, pattern = "Iran, Islamic Rep."), "Iran", no = country_name),
         country_name = ifelse(test = str_detect(string = country_name, pattern = "Korea, Dem. People’s Rep."), "North Korea", no = country_name),
         country_name = ifelse(test = str_detect(string = country_name, pattern = "Korea, Rep."), "South Korea", no = country_name),
         country_name = ifelse(test = str_detect(string = country_name, pattern = "Kyrgyz Republic"), "Kyrgyzstan", no = country_name),
         country_name = ifelse(test = str_detect(string = country_name, pattern = "Lao PDR"), "Laos", no = country_name),
         country_name = ifelse(test = str_detect(string = country_name, pattern = "Macao SAR, China"), "Macau SAR China", no = country_name),
         country_name = ifelse(test = str_detect(string = country_name, pattern = "Macedonia, FYR"), "Macedonia", no = country_name),
         country_name = ifelse(test = str_detect(string = country_name, pattern = "Micronesia, Fed. Sts."), 
                               "Micronesia (Federated States of)", no = country_name),
         country_name = ifelse(test = str_detect(string = country_name, pattern = "Myanmar"), "Myanmar (Burma)", no = country_name),
         country_name = ifelse(test = str_detect(string = country_name, pattern = "Russian Federation"), "Russia", no = country_name),
         country_name = ifelse(test = str_detect(string = country_name, pattern = "Sao Tome & Principe"), "São Tomé & Príncipe", no = country_name),
         country_name = ifelse(test = str_detect(string = country_name, pattern = "Sint Maarten (Dutch part)"), "Sint Maarten", no = country_name),
         country_name = ifelse(test = str_detect(string = country_name, pattern = "Slovak Republic"), "Slovakia", no = country_name),
         country_name = ifelse(test = str_detect(string = country_name, pattern = "St. Vincent & Grenadines"), 
                               "St. Vincent & the Grenadines", no = country_name),
         country_name = ifelse(test = str_detect(string = country_name, pattern = "Syrian Arab Republic"), "Syria", no = country_name),
         country_name = ifelse(test = str_detect(string = country_name, pattern = "Venezuela, RB"), "Venezuela", no = country_name),
         country_name = ifelse(test = str_detect(string = country_name, pattern = "Yemen, Rep."), "Yemen", no = country_name)) 

# now, we have to widen the data set and rename the variables to something more practical 
widened_df = filtered_df %>% 
  inner_join(all_diversity_analyses %>% distinct(country_name), by = "country_name") %>%
  mutate(indicator_name = ifelse(test = str_detect(string = indicator_name, 
          pattern = "Computer, communications and other services % of commercial service exports"), "prc_comp_exports", no = indicator_name),
         indicator_name = ifelse(test = str_detect(string = indicator_name, 
          pattern = "Computer, communications and other services % of commercial service imports"), "prc_comp_imports", no = indicator_name),
         indicator_name = ifelse(test = str_detect(string = indicator_name, 
          pattern = "CPIA gender equality rating 1=low to 6=high"), "gender_equality", no = indicator_name),
         indicator_name = ifelse(test = str_detect(string = indicator_name, 
          pattern = "CPIA policies for social inclusion/equity cluster average 1=low to 6=high"), "social_equality", no = indicator_name),
         indicator_name = ifelse(test = str_detect(string = indicator_name, 
          pattern = "CPIA trade rating 1=low to 6=high"), "trade_rating", no = indicator_name),
         indicator_name = ifelse(test = str_detect(string = indicator_name, 
          pattern = "GDP per capita constant 2010 US"), "gdp_per_capita", no = indicator_name),
         indicator_name = ifelse(test = str_detect(string = indicator_name, 
          pattern = "Gini index World Bank estimate"), "gini_index", no = indicator_name),
         indicator_name = ifelse(test = str_detect(string = indicator_name, 
          pattern = "High-technology exports % of manufactured exports"), "high_tech_exports", no = indicator_name),
         indicator_name = ifelse(test = str_detect(string = indicator_name, 
          pattern = "Human capital index HCI scale 0-1"), "human_capital_index", no = indicator_name),
         indicator_name = ifelse(test = str_detect(string = indicator_name, 
          pattern = "Individuals using the Internet % of population"), "prc_internet", no = indicator_name),
         indicator_name = ifelse(test = str_detect(string = indicator_name, 
          pattern = "Investment in ICT with private participation current US$"), "ict_investment", no = indicator_name),
         indicator_name = ifelse(test = str_detect(string = indicator_name, 
          pattern = "Research and development expenditure % of GDP"), "prc_rnd_expnd", no = indicator_name),
         indicator_name = ifelse(test = str_detect(string = indicator_name, 
          pattern = "Researchers in R&D per million people"), "rnd_rchrs_permil", no = indicator_name),
         indicator_name = ifelse(test = str_detect(string = indicator_name, 
          pattern = "Technicians in R&D per million people"), "rnd_techs_permil", no = indicator_name),
         indicator_name = ifelse(test = str_detect(string = indicator_name, 
          pattern = "Secure Internet servers per 1 million people"), "servers_permil", no = indicator_name),
         indicator_name = ifelse(test = str_detect(string = indicator_name, 
          pattern = "Trademark applications, resident, by count"), "trademark_apps", no = indicator_name)) %>%
  pivot_wider(names_from = indicator_name, values_from = count) %>% 
  left_join(all_diversity_analyses, by = c("country_name", "year")) %>% 
  select(country_name, year, net_diversity, louvain_range, fstgrdy_range, everything(), -louvain_comm, -fstgrdy_comm, -id)

# now we have to clean the dai_data 
dai_cleaned = dai_data %>% 
  clean_names() %>% 
  rename(country_name = country) %>% 
  mutate(year = str_replace(year, "x", ""),
         year = as.numeric(year),
         country_name = str_replace(country_name, " and ", " & ")) %>% 
  mutate(country_name = ifelse(test = str_detect(string = country_name, pattern = "Bahamas, The"), "Bahamas", no = country_name),
         country_name = ifelse(test = str_detect(string = country_name, pattern = "Congo, Rep."), "Congo - Brazzaville", no = country_name),
         country_name = ifelse(test = str_detect(string = country_name, pattern = "Congo, Dem. Rep."), "Congo - Kinshasa", no = country_name),
         country_name = ifelse(test = str_detect(string = country_name, pattern = "Cote d'Ivoire"), "Côte d’Ivoire", no = country_name),
         country_name = ifelse(test = str_detect(string = country_name, pattern = "Brunei Darussalam"), "Brunei", no = country_name),
         country_name = ifelse(test = str_detect(string = country_name, pattern = "Cabo Verde"), "Cape Verde", no = country_name),
         country_name = ifelse(test = str_detect(string = country_name, pattern = "Czech Republic"), "Czechia", no = country_name),
         country_name = ifelse(test = str_detect(string = country_name, pattern = "Egypt, Arab Rep."), "Egypt", no = country_name),
         country_name = ifelse(test = str_detect(string = country_name, pattern = "Gambia, The"), "Gambia", no = country_name),
         country_name = ifelse(test = str_detect(string = country_name, pattern = "Hong Kong SAR, China"), "Hong Kong SAR China", no = country_name),
         country_name = ifelse(test = str_detect(string = country_name, pattern = "Iran, Islamic Rep."), "Iran", no = country_name),
         country_name = ifelse(test = str_detect(string = country_name, pattern = "Korea, Dem. People’s Rep."), "North Korea", no = country_name),
         country_name = ifelse(test = str_detect(string = country_name, pattern = "Korea, Rep."), "South Korea", no = country_name),
         country_name = ifelse(test = str_detect(string = country_name, pattern = "Kyrgyz Republic"), "Kyrgyzstan", no = country_name),
         country_name = ifelse(test = str_detect(string = country_name, pattern = "Lao PDR"), "Laos", no = country_name),
         country_name = ifelse(test = str_detect(string = country_name, pattern = "Macedonia, FYR"), "Macedonia", no = country_name),
         country_name = ifelse(test = str_detect(string = country_name, pattern = "Macao SAR, China"), "Macau SAR China", no = country_name),
         country_name = ifelse(test = str_detect(string = country_name, pattern = "Micronesia, Fed. Sts."), 
                               "Micronesia (Federated States of)", no = country_name),
         country_name = ifelse(test = str_detect(string = country_name, pattern = "Myanmar"), "Myanmar (Burma)", no = country_name),
         country_name = ifelse(test = str_detect(string = country_name, pattern = "Russian Federation"), "Russia", no = country_name),
         country_name = ifelse(test = str_detect(string = country_name, pattern = "Sao Tome & Principe"), "São Tomé & Príncipe", no = country_name),
         country_name = ifelse(test = str_detect(string = country_name, pattern = "Sint Maarten (Dutch part)"), "Sint Maarten", no = country_name),
         country_name = ifelse(test = str_detect(string = country_name, pattern = "Slovak Republic"), "Slovakia", no = country_name),
         country_name = ifelse(test = str_detect(string = country_name, pattern = "St. Vincent & Grenadines"), 
                               "St. Vincent & the Grenadines", no = country_name),
         country_name = ifelse(test = str_detect(string = country_name, pattern = "Syrian Arab Republic"), "Syria", no = country_name),
         country_name = ifelse(test = str_detect(string = country_name, pattern = "Venezuela, RB"), "Venezuela", no = country_name),
         country_name = ifelse(test = str_detect(string = country_name, pattern = "Yemen, Rep."), "Yemen", no = country_name)) 

# lets join all of the data together 
widened_with_pop_df = widened_df %>% 
  left_join(population_cleaned, by = c("country_name", "year"))

widened_dai_df = widened_with_pop_df %>% 
  left_join(dai_cleaned, by = c("country_name", "year"))

# and normalize all of the oss data by the population size to get per_capita vars 
wdi_github_joined = ctry_github_joined %>% 
  left_join(widened_dai_df, by = c("country_name", "year")) %>% 
  drop_na(net_diversity) %>% 
  mutate(
    users_per_capita = (users / population)*100000, 
    repos_per_capita = (repos / population)*100000, 
    commits_per_capita = (commits / population)*100000,
    adds_per_capita = (additions / population)*100000,
    dels_per_capita = (deletions / population)*100000,
    collabs_per_capita = (total_collaborations / population)*100000,
    dom_collabs_per_capita = (domestic_collaborations / population)*100000,
    us_collabs_per_capita = (us_collaborations / population)*100000
    ) %>% 
  select(country_name, year, population, everything()) %>% 
  arrange(year, country_name)

wdi_github_joined
```

#### Checking Missing Data & Limiting the Features  

Let's use the `naniar` package to visualize how much missing data we have out of our ~2500 data points. 

```{r, fig.width=11}
gg_miss_var(wdi_github_joined)
```

Look's like there is a lot of missing data for some variables and a couple more that are just missing certain years in our dataset. We ended up having to remove most of the technology adoption variables, the human capital and equity variables, and only include data from 2009-2018. Still, we end up with around ~1300 data points and 18 features, though we ended up removing some of these features to avoid multicollinearity (see below). 

```{r, fig.width=11}
vis_miss(wdi_github_joined %>% 
           filter(year > 2009 & year < 2018) %>% 
           select(country_name, year, users_per_capita, repos_per_capita, commits_per_capita, adds_per_capita, dels_per_capita, 
                  collabs_per_capita, dom_collabs_per_capita, us_collabs_per_capita, net_diversity, fstgrdy_range,
                  gdp_per_capita, servers_permil, prc_internet, prc_comp_exports, prc_comp_imports #, high_tech_exports
                  #trademark_apps, prc_rnd_expnd, rnd_rchrs_permil, rnd_techs_permil, gini_index, everything(), 
                  #-users:-fstgrdy_range, -starts_with("dai_"), -human_capital_index, -ict_investment, -digital_adoption_index
                  ))
```

```{r}
curated_data <- wdi_github_joined %>% 
           filter(year > 2009 & year < 2018) %>% 
           select(country_name, year, users_per_capita, repos_per_capita, commits_per_capita, 
                  adds_per_capita, dels_per_capita, 
                  collabs_per_capita, dom_collabs_per_capita, us_collabs_per_capita, net_diversity, fstgrdy_range,
                  gdp_per_capita, servers_permil, prc_internet, prc_comp_exports, prc_comp_imports)
curated_data
```

Next, we will see how much missing data looks like. The blue and red boxplots should be mostly overlapping, which supports a Missing Completely at Random interpretation. This means we should be able to impute without worrying that something systematic is going on in our data. 

```{r}
pbox(curated_data, pos = 2)
```

#### Imputing the missing data 

Next, we are going to impute some of the missing data using the `MICE` package ([van Buuren 2011](https://cran.r-project.org/web/packages/mice/mice.pdf); [see also Alice 2018](https://datascienceplus.com/imputing-missing-data-with-r-mice-package/)). We decided to impute over 5 datasets and to use the classification and regression trees approach (`cart`) approach, which ensures that imputed values are plausible by allocating values that are deemed most appropriate "tree". This is similar to the predictive mean matching (`pmm`), which optimizes the imputed measure based on a regression line, but we needed to use the `cart` approach because of unbalanced classes. To do this, we used the `mice()` function, then add it back to the original data with `complete()`, and visualize our now non-missing data. 

```{r, fig.width=11}
tempData <- mice(curated_data, m=5, maxit=50, meth='cart', seed=500)
summary(tempData)
imputed_data <- complete(tempData,1)
vis_miss(imputed_data)
```

#### Transforming and Scaling the Data 

Ok, now let's look at the distributions of the variables. 

```{r}
melted_data <- melt(imputed_data)
ggplot(data = melted_data, aes(x = value)) + 
  stat_density() + 
  facet_wrap(~variable, scales = "free")
```

We can see that there are a lot of skewed distributions and that the majority of our data need to be transformed. 

```{r}
describe(imputed_data) %>% select(-vars, -n, -trimmed, -mad)
```

Indeed, the skewness and kurtosis are way off on almost all the variables. Let's apply a z-score standardization and some log transformations.

```{r}
ihs <- function(x) {
  y <- log(x + sqrt(x^2 + 1))
  return(y)
}

normalize <- function(x, na.rm = TRUE) {
    return((x- min(x)) /(max(x)-min(x)))
}
  
transformed_data <- imputed_data %>% 
  # transform the data first 
  mutate(
    users_per_capita = ihs(users_per_capita),
    repos_per_capita = ihs(repos_per_capita), 
    commits_per_capita = ihs(commits_per_capita), 
    adds_per_capita = ihs(adds_per_capita), 
    dels_per_capita = ihs(dels_per_capita), 
    collabs_per_capita = ihs(collabs_per_capita), 
    dom_collabs_per_capita = ihs(dom_collabs_per_capita), 
    us_collabs_per_capita = ihs(us_collabs_per_capita),
    gdp_per_capita = ihs(gdp_per_capita), 
    servers_permil = ihs(servers_permil)  
  ) %>% 
  mutate_at(vars(users_per_capita:prc_comp_imports), normalize) %>% 
  select(-fstgrdy_range)

transformed_data
```

```{r}
melted_data <- melt(transformed_data)
ggplot(data = melted_data, aes(x = value)) + 
  stat_density() + 
  facet_wrap(~variable, scales = "free") 
```

```{r}
describe(transformed_data) %>% select(-vars, -n, -trimmed, -mad)
```

Looks like those transformations got almost everthing within the correct skewness (+/-1) and kurtosis (+/-3) ranges other than the network_range measure, which is so bimodal (0 or >0.8) that none of the common transformations helped convert it to anything close to a Gaussian distribution. We just won't use that variable.

# Now, let's take a look at the correlation matrix of our variables and visualize that as a heatmap. 

```{r}
cor_matrix <- as.data.frame(cor(na.omit(transformed_data %>% select(-country_name, -year) , method = "pearson")))
cor_matrix <- cor_matrix %>% mutate(across(where(is.numeric), round, 3))
cor_matrix
```

```{r}
melted_cormat <- cor_matrix %>% 
  rownames_to_column("Var1") %>% 
  pivot_longer(!Var1, names_to = "Var2", values_to = "value") 

ggplot(data = melted_cormat, aes(x=Var1, y=Var2, fill=value)) + 
  geom_tile() + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

#### Fitting Models 

##### Linear Model

Now, we will fit a linear model. Let's add the number of users, domestic collaborations, and the tech usage and economy variables. 

```{r}
fit1 <- lm(collabs_per_capita ~ users_per_capita + commits_per_capita + dom_collabs_per_capita + us_collabs_per_capita +
            gdp_per_capita + servers_permil + prc_internet + prc_comp_exports + prc_comp_imports + net_diversity, data=transformed_data)
summary(fit1) 
car::vif(fit1)
```

The R-squared looks good and lots of stars, but.... also lots of multicollinearity in here (several variables exceeding a VIF of 5).

Overall, the model seems to fit pretty well but we should double-check all our assumptions to make sure this model is alright ([see this for an overview](http://www.sthda.com/english/articles/39-regression-model-diagnostics/161-linear-regression-assumptions-and-diagnostics-in-r-essentials/)).  

```{r, fig.width=11}
par(mfrow = c(2, 2))
plot(fit1)
```

Looks like there are all kinds of violations of our assumptions, which probably means that a linear model is not the most appropriate selection. Let's try another model that accounts for the skewedness of our outcome variable. 

```{r}
fit2 <- glm(collabs_per_capita ~ users_per_capita + commits_per_capita + dom_collabs_per_capita + us_collabs_per_capita +
            gdp_per_capita + servers_permil + prc_internet + prc_comp_exports + prc_comp_imports + net_diversity, 
            family = inverse.gaussian(link = "1/mu^2"), data=transformed_data)
summary(fit2) 
car::vif(fit2)
```

```{r, fig.width=11}
par(mfrow = c(2, 2))
plot(fit2)
```
```{r}
cutoff <- 4/((nrow(transformed_data)-length(fit2$coefficients)-2)) 
plot(fit2, which=4, cook.levels=cutoff) 
```

```{r}
cooksd <- cooks.distance(fit2)
influential <- as.numeric(names(cooksd)[(cooksd > (4/1356))])
transformed_influential <- transformed_data[-influential,]
```

```{r}
fit3 <- glm(collabs_per_capita ~ users_per_capita + commits_per_capita + dom_collabs_per_capita + us_collabs_per_capita +
            gdp_per_capita + servers_permil + prc_internet + prc_comp_exports + prc_comp_imports + net_diversity, 
            family = gaussian(link = "identity"), data=transformed_data)
summary(fit3) 
car::vif(fit3)
```
```{r}
cutoff <- 4/((nrow(transformed_influential)-length(fit3$coefficients)-2)) 
plot(fit3, which=4, cook.levels=cutoff) 
```

# STARTING HERE WITH NEGATIVE BINOMIAL 

```{r}
fit_nb <- MASS::glm.nb(collabs_per_capita ~ users_per_capita + commits_per_capita + dom_collabs_per_capita + us_collabs_per_capita +
            gdp_per_capita + servers_permil + prc_internet + prc_comp_exports + prc_comp_imports + net_diversity, 
            data = imputed_data)
summary(fit_nb)
car::vif(fit_nb)
```

```{r, fig.width=11}
par(mfrow = c(2, 2))
plot(fit_nb)
```
```{r}
cooksd <- cooks.distance(fit_nb)
sample_size <- nrow(imputed_data)
plot(cooksd, pch="*", cex=2, main="Influential Obs by Cooks distance")
abline(h = 4/sample_size, col="red")
text(x=1:length(cooksd)+1, y=cooksd, labels=ifelse(cooksd>4/sample_size, names(cooksd),""), col="red")
```
```{r}
influential <- as.numeric(names(cooksd)[(cooksd > (4/sample_size))])
imputed_data_inf <- imputed_data[-influential, ]
fit_nb_inf <- MASS::glm.nb(collabs_per_capita ~ users_per_capita + commits_per_capita + 
                             dom_collabs_per_capita + us_collabs_per_capita + gdp_per_capita + 
                             servers_permil + prc_internet + prc_comp_exports + prc_comp_imports + net_diversity, 
            data = imputed_data_inf)
summary(fit_nb_inf)
car::vif(fit_nb_inf)
```

```{r, fig.width=11}
par(mfrow = c(2, 2))
plot(fit_nb_inf)
```







https://stats.idre.ucla.edu/r/dae/negative-binomial-regression/

```{r}
m3 <- glm(collabs_per_capita ~ users_per_capita + commits_per_capita + 
                             dom_collabs_per_capita + us_collabs_per_capita + gdp_per_capita + 
                             servers_permil + prc_internet + prc_comp_exports + prc_comp_imports + net_diversity, 
            family = "poisson", data = imputed_data_inf)
pchisq(2 * (logLik(fit_nb_inf) - logLik(m3)), df = 1, lower.tail = FALSE)
```
```{r}
(est <- cbind(Estimate = coef(fit_nb_inf), confint(fit_nb_inf)))
```
```{r}
exp(est)
```

```{r}
newdata1 <- data.frame(collabs_per_capita = mean(imputed_data_inf$collabs_per_capita), 
                       users_per_capita = mean(imputed_data_inf$users_per_capita),
                       commits_per_capita = mean(imputed_data_inf$commits_per_capita), 
                       collabs_per_capita = mean(imputed_data_inf$collabs_per_capita), 
                       us_collabs_per_capita = mean(imputed_data_inf$us_collabs_per_capita),
                       dom_collabs_per_capita = mean(imputed_data_inf$dom_collabs_per_capita), 
                       gdp_per_capita = mean(imputed_data_inf$gdp_per_capita),  
                       servers_permil = mean(imputed_data_inf$servers_permil),
                       prc_internet = mean(imputed_data_inf$prc_internet), 
                       prc_comp_exports = mean(imputed_data_inf$prc_comp_exports), 
                       prc_comp_imports  = mean(imputed_data_inf$prc_comp_imports ),
                       net_diversity = mean(imputed_data_inf$net_diversity))
newdata1$phat <- predict(fit_nb_inf, newdata1, type = "response")
newdata1
```
```{r}
ggplot(newdata1, aes(collabs_exp, users_per_capita)) +
  geom_ribbon(aes(ymin = LL, ymax = UL, fill = prog), alpha = .25) +
  geom_line(aes(colour = prog), size = 2) +
  labs(x = "Collabs Per Capita", y = "Users Per Capita")

```



#### References 

https://ramorel.github.io/network-range/

https://cran.r-project.org/web/packages/intergraph/vignettes/howto.html

http://www.sthda.com/english/articles/39-regression-model-diagnostics/160-multicollinearity-essentials-and-vif-in-r/

http://www.sthda.com/english/articles/39-regression-model-diagnostics/161-linear-regression-assumptions-and-diagnostics-in-r-essentials/

### World Technology Data 

https://datacatalog.worldbank.org/dataset/world-development-indicators

https://www.worldbank.org/en/publication/wdr2016/Digital-Adoption-Index

https://www.nber.org/research/data/historical-cross-country-technology-adoption-hccta-dataset






























