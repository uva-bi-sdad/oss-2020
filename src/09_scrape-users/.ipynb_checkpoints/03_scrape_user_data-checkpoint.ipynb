{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load packages \n",
    "import os\n",
    "import psycopg2 as pg\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "import requests as r\n",
    "import string \n",
    "import json\n",
    "import base64\n",
    "import urllib.request\n",
    "import itertools \n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "from github import Github, RateLimitExceededException, BadCredentialsException, BadAttributeException, GithubException, UnknownObjectException, BadUserAgentException\n",
    "import warnings\n",
    "import datetime\n",
    "\n",
    "import multiprocessing\n",
    "#from multiprocessing.pool import ThreadPool as Pool\n",
    "from multiprocessing import Pool, freeze_support\n",
    "\n",
    "import concurrent.futures\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3260612\n",
      "kerneq sosterbind\n"
     ]
    }
   ],
   "source": [
    "# connect to the database, download data, limit to repos with at least 20,000 commits?\n",
    "connection = pg.connect(host = 'postgis1', database = 'sdad', \n",
    "                        user = os.environ.get('db_user'), \n",
    "                        password = os.environ.get('db_pwd'))\n",
    "\n",
    "#PATs access token, saved as a dataframe\n",
    "github_pats = '''SELECT * FROM gh_2007_2020.pats_update'''\n",
    "github_pats = pd.read_sql_query(github_pats, con=connection)\n",
    "access_tokens = github_pats[\"token\"]\n",
    "num_token = '''SELECT COUNT(*) FROM gh_2007_2020.pats_update'''\n",
    "num_token = pd.read_sql_query(num_token, con=connection)\n",
    "num_token=num_token.iloc[0]['count']\n",
    "\n",
    "# grab distinct users \n",
    "distinct_users = '''SELECT * FROM gh.ctrs_from_commits_lchn'''\n",
    "distinct_users = pd.read_sql_query(distinct_users, con=connection)\n",
    "#get rid of leading and ending space, save users to a list\n",
    "users_list = distinct_users[\"login\"].tolist()\n",
    "logins = []\n",
    "for s in users_list:\n",
    "    logins.append(s.strip())  \n",
    "print(len(logins))\n",
    "print(logins[0], logins[len(logins)-1])\n",
    "\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index ranges from 0 to maximum number of PATs available\n",
    "def get_access_token(github_pat_index):\n",
    "    if github_pat_index < num_token:\n",
    "        return github_pats.token[github_pat_index]\n",
    "    else:\n",
    "        print(\"token exceed limit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_user_stats(users, github_pat_index):\n",
    "    df_user_stats = pd.DataFrame()\n",
    "    for user in users:\n",
    "        if github_pat_index >= len(access_tokens):\n",
    "            github_pat_index -= len(access_tokens)\n",
    "            print(\"***PAT access token exceed limit, restart access token loop with #\", github_pat_index)\n",
    "        while github_pat_index < len(access_tokens):\n",
    "            try:\n",
    "                access_token = get_access_token(github_pat_index)\n",
    "                #print(\"Scrapping --\", user,\". Extracting access token #\", github_pat_index+1,\", total\", num_token, \"tokens are available.\")\n",
    "                #if false, retry until true, max number of retry is 20 times\n",
    "                g = Github(access_token, retry = 20, timeout = 15)\n",
    "                user = g.get_user(user)\n",
    "                df_user_stats = df_user_stats.append({\n",
    "                    \"login\": user.login, \"name\": user.name, \"email\": user.email, \"bio\": user.bio,\n",
    "                    \"company\": user.company, \"location\": user.location,\n",
    "                    \"created_at\": user.created_at.strftime(\"%m/%d/%Y %H:%M:%S\"),\n",
    "                    \"blog\": user.blog, \"orgs_url\": user.organizations_url,\n",
    "                    \"collaborators\": user.collaborators, \"repos_url\": user.repos_url,\n",
    "                    \"gists_public\": user.public_gists, \"gists_private\": user.private_gists,\n",
    "                    \"repos_public\": user.public_repos, \"repos_private\": user.total_private_repos,\n",
    "                    \"followers\": user.followers, \"followers_url\": user.followers_url,\n",
    "                    \"following\": user.following, \"following_url\": user.following_url\n",
    "                }, ignore_index = True)\n",
    "            except RateLimitExceededException as e:\n",
    "                print(e.status)\n",
    "                print('Rate limit exceeded --', user, \", using access token #\", github_pat_index)\n",
    "                print(\"Current time:\", datetime.datetime.now())\n",
    "                #time.sleep(300)\n",
    "                github_pat_index+=1\n",
    "                print(\"***Exit current access token, proceed with next aceess token #\", github_pat_index, \"rescrape --\",user)\n",
    "                break\n",
    "            except BadCredentialsException as e:\n",
    "                print(e.status)\n",
    "                print('Bad credentials exception --', user, \", using access token #\", github_pat_index)\n",
    "                print(\"Current time:\", datetime.datetime.now())\n",
    "                github_pat_index+=1\n",
    "                print(\"***Exit current access token, proceed with next aceess token #\", github_pat_index, \"rescrape --\",user)\n",
    "                break\n",
    "            except UnknownObjectException as e:\n",
    "                print(e.status)\n",
    "                print('Unknown object exception --', user)\n",
    "                break\n",
    "            except GithubException as e:\n",
    "                print(e.status)\n",
    "                print('General exception --', user)\n",
    "                break\n",
    "            except r.exceptions.ConnectionError as e:\n",
    "                print('Retries limit exceeded --', user)\n",
    "                print(str(e))\n",
    "                time.sleep(10)\n",
    "                continue\n",
    "            except r.exceptions.Timeout as e:\n",
    "                print('Time out exception --', user)\n",
    "                print(str(e))\n",
    "                time.sleep(10)\n",
    "                continue\n",
    "            break\n",
    "    return df_user_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49999"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0-200k done but need to revalidate by filtering vs already scraped\n",
    "logins_subset = logins[3200001:3250000]\n",
    "len(logins_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.datetime.now()\n",
    "print(\"Start scraping:\", start_time)\n",
    "#specify the index of pat you want use to start scraping\n",
    "df_user_stats = pull_user_stats(logins_subset, 20) \n",
    "df_user_stats = df_user_stats[['login', 'name', 'email', 'bio', 'company', 'location','created_at','blog',\n",
    "                               'orgs_url','collaborators','gists_public','gists_private','repos_public',\n",
    "                               'repos_private','repos_url','followers','followers_url','following','following_url']]\n",
    "end_time =  datetime.datetime.now()\n",
    "df_user_stats.to_csv(r'/project/biocomplexity/sdad/projects_data/ncses/oss/github_user_data/77_github_user_data.csv', index = False)\n",
    "print(\"Finished scraping\", len(df_user_stats), \"of\", len(logins_subset), \"records at\", end_time)\n",
    "print(\"It took\", end_time-start_time, \"to run.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user_stats = pd.read_csv('/home/kb7hp/git/dspg21oss/data/dspg21oss/02_github_user_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "login            0\n",
       "name             0\n",
       "email            0\n",
       "bio              0\n",
       "company          0\n",
       "location         0\n",
       "created_at       0\n",
       "blog             0\n",
       "orgs_url         0\n",
       "collaborators    0\n",
       "gists_public     0\n",
       "gists_private    0\n",
       "repos_public     0\n",
       "repos_private    0\n",
       "repos_url        0\n",
       "followers        0\n",
       "followers_url    0\n",
       "following        0\n",
       "following_url    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "already_scraped = df_user_stats['login'].tolist()\n",
    "already_scraped = ~df_user_stats.login.isin(already_scraped)\n",
    "df_user_stats = df_user_stats[already_scraped]\n",
    "df_user_stats.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user_stats.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Series.unique of 0                 Arro\n",
       "1            Surge1223\n",
       "2            matcarter\n",
       "3             nirajsha\n",
       "4        jkarnasiewicz\n",
       "             ...      \n",
       "48693      claude-zhou\n",
       "48694    shixiaohu2206\n",
       "48695          L1cardo\n",
       "48696    dfernandezlop\n",
       "48697         ptsneves\n",
       "Name: login, Length: 3203867, dtype: object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import glob \n",
    "os.chdir('/project/biocomplexity/sdad/projects_data/ncses/oss/github_user_data/')\n",
    "all_filenames = [i for i in glob.glob('*_github_user_data.csv')]\n",
    "combined_csv = pd.concat([pd.read_csv(f) for f in all_filenames ])\n",
    "combined_csv['login'].unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "login    96924\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "already_scraped = combined_csv['login'].tolist()\n",
    "already_scraped = ~distinct_users.login.isin(already_scraped)\n",
    "distinct_users = distinct_users[already_scraped]\n",
    "distinct_users.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64763\n"
     ]
    }
   ],
   "source": [
    "users_list = distinct_users[\"login\"].tolist()\n",
    "logins_leftovers = []\n",
    "for s in users_list:\n",
    "    logins_leftovers.append(s.strip())  \n",
    "logins_leftovers = logins_leftovers[130001:195000]\n",
    "print(len(logins_leftovers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.datetime.now()\n",
    "print(\"Start scraping:\", start_time)\n",
    "#specify the index of pat you want use to start scraping\n",
    "df_user_stats = pull_user_stats(logins_leftovers, 20) \n",
    "df_user_stats = df_user_stats[['login', 'name', 'email', 'bio', 'company', 'location','created_at','blog',\n",
    "                               'orgs_url','collaborators','gists_public','gists_private','repos_public',\n",
    "                               'repos_private','repos_url','followers','followers_url','following','following_url']]\n",
    "df_user_stats.to_csv(r'/project/biocomplexity/sdad/projects_data/ncses/oss/github_user_data/81_github_user_data.csv', index = False)\n",
    "end_time =  datetime.datetime.now()\n",
    "print(\"Finished scraping\", len(df_user_stats), \"of\", len(logins_leftovers), \"records at\", end_time)\n",
    "print(\"It took\", end_time-start_time, \"to run.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-brandon_env]",
   "language": "python",
   "name": "conda-env-.conda-brandon_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "toc-showcode": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
